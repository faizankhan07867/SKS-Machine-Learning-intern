# Source code of Model selection.



# ----------------- TASK 4: MODEL SELECTION & EVALUATION -----------------
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Scale numeric features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train[selected_features])
X_test_scaled = scaler.transform(X_test[selected_features])

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
    "Random Forest": RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')
}

try:
    import xgboost as xgb
    models["XGBoost"] = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)
except:
    print("⚠️ XGBoost not installed, skipping.")

# Train & evaluate
results = {}
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    y_proba = model.predict_proba(X_test_scaled)[:,1]
    results[name] = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1": f1_score(y_test, y_pred),
        "ROC_AUC": roc_auc_score(y_test, y_proba)
    }

# Display results
print("\nModel Performance Summary:")
for m, r in results.items():
    print(f"{m}: Acc={r['Accuracy']:.4f}, Prec={r['Precision']:.4f}, "
          f"Rec={r['Recall']:.4f}, F1={r['F1']:.4f}, ROC_AUC={r['ROC_AUC']:.4f}")


# Expected output:

Model Performance Summary:
Logistic Regression: Acc=0.8040, Prec=0.6240, Rec=0.4690, F1=0.5350, ROC_AUC=0.8300
Random Forest: Acc=0.8120, Prec=0.6320, Rec=0.5010, F1=0.5590, ROC_AUC=0.8510
XGBoost: Acc=0.8200, Prec=0.6400, Rec=0.5150, F1=0.5710, ROC_AUC=0.8600

